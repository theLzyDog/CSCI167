{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2U/x+73e7F5bhA+qc9dFF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theLzyDog/CSCI167/blob/main/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "po4m1PvvAvcp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7128a894",
        "outputId": "115e0816-92f3-4823-9f30-2c88d91dae44"
      },
      "source": [
        "pip install datasets"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d711c7dc",
        "outputId": "8de3684d-3a44-4d77-95af-e224671bf907"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('microsoft/cats_vs_dogs')\n",
        "print(dataset)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['image', 'labels'],\n",
            "        num_rows: 23410\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nsZ7--ysARzl"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ea88ed1"
      },
      "source": [
        "# Task\n",
        "Define image transformations for the `cats_vs_dogs` dataset including resizing, conversion to tensor, and normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77668165"
      },
      "source": [
        "## Define Image Transformations\n",
        "\n",
        "### Subtask:\n",
        "Define the necessary image transformations (e.g., resizing, converting to tensor, normalization) for the input to the CNN using `torchvision.transforms`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6e065ce"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define image transformations as specified in the instructions. This involves importing `torchvision.transforms`, composing a transformation pipeline with `Resize`, `ToTensor`, and `Normalize`, and storing it in a variable named `transform`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4c60c7f",
        "outputId": "3a5048d0-f725-4273-f6c7-041dd9118ca7"
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\"Image transformation pipeline defined.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image transformation pipeline defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de683cd0"
      },
      "source": [
        "## Preprocess Dataset and Create DataLoaders\n",
        "\n",
        "### Subtask:\n",
        "Apply the defined transformations to the `cats_vs_dogs` dataset. Split the dataset into training and validation sets. Create `DataLoader` instances for efficient batching of the data during training and evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bef3093c"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define a function to apply the `transform` to each image in the dataset, which is necessary before splitting and loading the data. This function will be mapped over the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df46e6e4",
        "outputId": "33afb26a-55d9-4df5-a11b-6c60a65cc22a"
      },
      "source": [
        "def apply_transforms(batch):\n",
        "    batch['image'] = [transform(img) for img in batch['image']]\n",
        "    return batch\n",
        "\n",
        "print(\"Transformation application function defined.\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformation application function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a3bef77"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the transformation function is defined, I will apply it to the dataset using `with_transform`, then split the dataset into training and validation sets, and finally create DataLoader instances for both.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "169cc579",
        "outputId": "53e0cc3c-9b93-4503-f9b5-48424bdc56df"
      },
      "source": [
        "transformed_dataset = dataset['train'].with_transform(apply_transforms)\n",
        "\n",
        "splits = transformed_dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = splits['train']\n",
        "val_dataset = splits['test']\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Number of training batches: {len(train_dataloader)}\")\n",
        "print(f\"Number of validation batches: {len(val_dataloader)}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 18728\n",
            "Validation dataset size: 4682\n",
            "Number of training batches: 293\n",
            "Number of validation batches: 74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "604b4464"
      },
      "source": [
        "## Define CNN Model Architecture\n",
        "\n",
        "### Subtask:\n",
        "Implement a simple Convolutional Neural Network (CNN) using `torch.nn.Module`. This will include convolutional layers, pooling layers, and fully connected layers for classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8c142ea"
      },
      "source": [
        "**Reasoning**:\n",
        "To implement the CNN model, I will define a class `SimpleCNN` that inherits from `torch.nn.Module`. This class will contain the convolutional, pooling, and fully connected layers as specified, along with the `forward` pass definition.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc6e1115",
        "outputId": "eafd58d9-7c0f-4e1d-b8f4-4180526855c9"
      },
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1), # Input: 3 channels (RGB), Output: 32 channels\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),    # Output size: 112x112\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # Input: 32 channels, Output: 64 channels\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),    # Output size: 56x56\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1), # Input: 64 channels, Output: 128 channels\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),    # Output size: 28x28\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1), # Input: 128 channels, Output: 256 channels\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)     # Output size: 14x14\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 14 * 14, 512),            # Fully connected layer\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)               # Output layer for 2 classes (cat or dog)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "print(\"SimpleCNN model architecture defined.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleCNN model architecture defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abf341ea"
      },
      "source": [
        "## Initialize Model, Loss Function, and Optimizer\n",
        "\n",
        "### Subtask:\n",
        "Instantiate the defined CNN model. Define the loss function (e.g., `nn.CrossEntropyLoss`) and an optimizer (e.g., `torch.optim.Adam`) for training the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7c78f48"
      },
      "source": [
        "**Reasoning**:\n",
        "To instantiate the model, define the loss function, and optimizer, I will write a code block that creates an instance of `SimpleCNN`, sets up `nn.CrossEntropyLoss` and `torch.optim.Adam`, and moves the model to the appropriate device (GPU if available, otherwise CPU).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1fa6573",
        "outputId": "d5235b87-cab1-4f54-bc43-8e680b23a46f"
      },
      "source": [
        "model = SimpleCNN(num_classes=2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model initialized and moved to {device}.\")\n",
        "print(f\"Loss function: {criterion}\")\n",
        "print(f\"Optimizer: {optimizer}\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized and moved to cpu.\n",
            "Loss function: CrossEntropyLoss()\n",
            "Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17f96e7a"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "### Subtask:\n",
        "Implement the training loop to train the CNN model over several epochs. This will involve iterating through the training DataLoader, performing forward passes, calculating loss, backpropagating gradients, and updating model weights. Include basic validation during training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "437c3006"
      },
      "source": [
        "**Reasoning**:\n",
        "To implement the training loop as described, I need to write a Python code block that iterates through a specified number of epochs, performs training and validation steps, calculates loss and accuracy, and stores these metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f864431f"
      },
      "source": [
        "num_epochs = 10\n",
        "\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        images = batch['image'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_train_loss = running_train_loss / len(train_dataloader)\n",
        "    epoch_train_accuracy = 100 * correct_train / total_train\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    train_accuracies.append(epoch_train_accuracy)\n",
        "\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(val_dataloader):\n",
        "            images = batch['image'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_val_loss = running_val_loss / len(val_dataloader)\n",
        "    epoch_val_accuracy = 100 * correct_val / total_val\n",
        "    val_losses.append(epoch_val_loss)\n",
        "    val_accuracies.append(val_accuracies)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "          f'Train Loss: {epoch_train_loss:.4f}, '\n",
        "          f'Train Acc: {epoch_train_accuracy:.2f}%, '\n",
        "          f'Val Loss: {epoch_val_loss:.4f}, '\n",
        "          f'Val Acc: {epoch_val_accuracy:.2f}%')\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af5a9751"
      },
      "source": [
        "**Reasoning**:\n",
        "The `RuntimeError` indicates that some images are grayscale (1 channel) while the normalization step expects 3 channels. To address this, I will add `transforms.Lambda(lambda image: image.convert(\"RGB\"))` to the transformation pipeline in cell `e4c60c7f` to ensure all images are converted to RGB format before normalization. This modification will prevent the channel mismatch error during the training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aa376e6",
        "outputId": "172b3a88-73fe-4118-8265-497ec9d17056"
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Lambda(lambda image: image.convert(\"RGB\")), # Add this line to ensure 3 channels\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\"Image transformation pipeline defined.\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image transformation pipeline defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "821c5834"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `transform` pipeline has been updated to handle potential grayscale images, the `apply_transforms` function needs to be re-defined and re-executed to ensure it uses the corrected `transform`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6530c98e",
        "outputId": "4464d84a-ad6b-484a-a57f-ac406e8e85ff"
      },
      "source": [
        "def apply_transforms(batch):\n",
        "    batch['image'] = [transform(img) for img in batch['image']]\n",
        "    return batch\n",
        "\n",
        "print(\"Transformation application function defined.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformation application function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3d3107e"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `apply_transforms` function has been re-defined with the updated `transform` pipeline, the `DataLoader` instances (`train_dataloader` and `val_dataloader`) must be re-created to incorporate these changes. This ensures that all images processed by the DataLoaders will use the corrected RGB transformation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8476bad",
        "outputId": "d2df943c-057c-4dc8-e44a-4956f4ab2959"
      },
      "source": [
        "transformed_dataset = dataset['train'].with_transform(apply_transforms)\n",
        "\n",
        "splits = transformed_dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = splits['train']\n",
        "val_dataset = splits['test']\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Number of training batches: {len(train_dataloader)}\")\n",
        "print(f\"Number of validation batches: {len(val_dataloader)}\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 18728\n",
            "Validation dataset size: 4682\n",
            "Number of training batches: 293\n",
            "Number of validation batches: 74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3053a836"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `transform` pipeline has been updated, and the `apply_transforms` function and `DataLoader` instances have been re-created to use the corrected transformations, I will re-execute the training loop. This should resolve the `RuntimeError` related to image channel mismatch and allow the model to train as intended.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a650338",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b9bfd45-c3d4-4163-857b-c367077dc285"
      },
      "source": [
        "num_epochs = 10\n",
        "\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        images = batch['image'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        print(f'{i} ')\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_train_loss = running_train_loss / len(train_dataloader)\n",
        "    epoch_train_accuracy = 100 * correct_train / total_train\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    train_accuracies.append(epoch_train_accuracy)\n",
        "\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(val_dataloader):\n",
        "            images = batch['image'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "            print(f'{i}/{i+1} ')\n",
        "\n",
        "    epoch_val_loss = running_val_loss / len(val_dataloader)\n",
        "    epoch_val_accuracy = 100 * correct_val / total_val\n",
        "    val_losses.append(epoch_val_loss)\n",
        "    val_accuracies.append(epoch_val_accuracy)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "          f'Train Loss: {epoch_train_loss:.4f}, '\n",
        "          f'Train Acc: {epoch_train_accuracy:.2f}%, '\n",
        "          f'Val Loss: {epoch_val_loss:.4f}, '\n",
        "          f'Val Acc: {epoch_val_accuracy:.2f}%')\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 \n",
            "1 \n",
            "2 \n",
            "3 \n",
            "4 \n",
            "5 \n",
            "6 \n",
            "7 \n",
            "8 \n",
            "9 \n",
            "10 \n",
            "11 \n"
          ]
        }
      ]
    }
  ]
}